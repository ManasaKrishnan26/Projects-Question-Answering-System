{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nimport nltk\nimport torch\nimport json\nimport string\nimport re\nfrom transformers import AutoTokenizer, AdamW, BertForQuestionAnswering, BertConfig\nfrom torch.utils.data import DataLoader\nfrom nltk.corpus import stopwords","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-16T19:51:06.679738Z","iopub.execute_input":"2023-04-16T19:51:06.680189Z","iopub.status.idle":"2023-04-16T19:51:06.688313Z","shell.execute_reply.started":"2023-04-16T19:51:06.680153Z","shell.execute_reply":"2023-04-16T19:51:06.686751Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = torch.load('/kaggle/input/finetunedmodel/finetunedmodel.h5', map_location = torch.device('cpu'))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T19:50:40.678490Z","iopub.execute_input":"2023-04-16T19:50:40.678993Z","iopub.status.idle":"2023-04-16T19:50:41.181730Z","shell.execute_reply.started":"2023-04-16T19:50:40.678952Z","shell.execute_reply":"2023-04-16T19:50:41.180011Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n# stop_words = set(stopwords.words('english'))\n\ndef preprocess(message):\n    message = message.lower()\n    punct = set(string.punctuation)\n    list_of_words = [i for i in message if i not in punct]\n    message = \"\".join(list_of_words)\n    articles = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n    message = re.sub(articles, \" \", message)\n    message = \" \".join(message.split())\n    return message\n#     message_temp = []\n#     for i in message.split():\n#         if i not in stop_words:\n#             message_temp.append(i)\n#     return \"\".join(message_temp)\n\ndef return_answer(paragraph, question):\n    inputs = tokenizer.encode_plus(question, paragraph, return_tensors = 'pt')\n    outputs = model(**inputs)\n    start_idx = torch.argmax(outputs[0])\n    end_idx = torch.argmax(outputs[1])\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx: end_idx])\n    predicted_answer = tokenizer.convert_tokens_to_string(tokens)\n    return predicted_answer\n\ndef calculate_f1(pred, y):\n    list_y = preprocess(y).split()\n    list_pred = preprocess(pred).split()\n    common = set(list_y) & set(list_pred)\n    if len(common) == 0:\n        return 0\n    \n    precision = len(common)/len(list_pred)\n    recall = len(common)/len(list_y)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\ndef predict_answer(context, question, true_answer):\n    predicted_answer = return_answer(context, question)\n    f1 = calculate_f1(predicted_answer, true_answer)\n    print(\"Question:\", question)\n    print(\"Predicted Answer:\", predicted_answer)\n    print(\"True Answer:\", true_answer)\n    print(\"F1:\", f1)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T19:51:50.289995Z","iopub.execute_input":"2023-04-16T19:51:50.290420Z","iopub.status.idle":"2023-04-16T19:51:50.506285Z","shell.execute_reply.started":"2023-04-16T19:51:50.290383Z","shell.execute_reply":"2023-04-16T19:51:50.504942Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"context = \"\"\" Mount Olympus is the highest mountain in Greece. It is part of the Olympus massif near \n              the Gulf of Thérmai of the Aegean Sea, located in the Olympus Range on the border between \n              Thessaly and Macedonia, between the regional units of Pieria and Larissa, about 80 km (50 mi) \n              southwest from Thessaloniki. Mount Olympus has 52 peaks and deep gorges. The highest peak, \n              Mytikas, meaning \"nose\", rises to 2917 metres (9,570 ft). It is one of the \n              highest peaks in Europe in terms of topographic prominence. \"\"\"\n\nqueries = [\n           \"Where Olympus is near?\",\n           \"How far away is Olympus from Thessaloniki?\"\n          ]\nanswers = [\n           \"Gulf of Thérmai of the Aegean Sea\",\n           \"80 km (50 mi)\"\n          ]\n\nfor question, answer in zip(queries,answers):\n    print(\"\\n\")\n    predict_answer(context, question, answer)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T20:00:16.080547Z","iopub.execute_input":"2023-04-16T20:00:16.081086Z","iopub.status.idle":"2023-04-16T20:00:16.769000Z","shell.execute_reply.started":"2023-04-16T20:00:16.081043Z","shell.execute_reply":"2023-04-16T20:00:16.767413Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\n\nQuestion: Where Olympus is near?\nPredicted Answer: gulf of therma\nTrue Answer: Gulf of Thérmai of the Aegean Sea\nF1: 0.4444444444444444\n\n\nQuestion: How far away is Olympus from Thessaloniki?\nPredicted Answer: 80 km ( 50 mi )\nTrue Answer: 80 km (50 mi)\nF1: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"context = \"\"\" Harry Potter is a series of seven fantasy novels written by British author, J. K. Rowling. The novels chronicle the lives of a young wizard, \n              Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. \n              The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard \n              governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people). Since the release of the first novel, \n              Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide. \n              They have attracted a wide adult audience as well as younger readers and are often considered cornerstones of modern young adult literature.[2] \n              As of February 2018, the books have sold more than 500 million copies worldwide, making them the best-selling book series in history, and have been translated \n              into eighty languages.[3] The last four books consecutively set records as the fastest-selling books in history, with the final installment selling roughly \n              eleven million copies in the United States within twenty-four hours of its release.  \"\"\"\n\nqueries = [\n           \"Who wrote Harry Potter's novels?\",\n           \"Who are Harry Potter's friends?\",\n           \"Which is the name of Harry Poter's first novel?\",\n           \"When did the first novel release?\"\n          ]\nanswers = [\n           \"J. K. Rowling\",\n           \"Hermione Granger and Ron Weasley\",\n           \"Harry Potter and the Philosopher's Stone\",\n           \"26 June 1997\"\n          ]\n\nfor question, answer in zip(queries,answers):\n    print(\"\\n\")\n    predict_answer(context, question, answer)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T20:02:17.391376Z","iopub.execute_input":"2023-04-16T20:02:17.392691Z","iopub.status.idle":"2023-04-16T20:02:20.189392Z","shell.execute_reply.started":"2023-04-16T20:02:17.392631Z","shell.execute_reply":"2023-04-16T20:02:20.187723Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"\n\nQuestion: Who wrote Harry Potter's novels?\nPredicted Answer: j. k. rowling\nTrue Answer: J. K. Rowling\nF1: 1.0\n\n\nQuestion: Who are Harry Potter's friends?\nPredicted Answer: hermione granger and ron weasley\nTrue Answer: Hermione Granger and Ron Weasley\nF1: 1.0\n\n\nQuestion: Which is the name of Harry Poter's first novel?\nPredicted Answer: harry potter and the philosopher ' s stone\nTrue Answer: Harry Potter and the Philosopher's Stone\nF1: 0.7272727272727272\n\n\nQuestion: When did the first novel release?\nPredicted Answer: 26 june 1997\nTrue Answer: 26 June 1997\nF1: 1.0\n","output_type":"stream"}]}]}