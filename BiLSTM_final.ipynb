{
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "     \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "import json\n",
        "from transformers import AutoTokenizer, BertModel,DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkoKMxZVUwDC",
        "outputId": "4a9730e8-95ae-4d9b-ae02-009dfd3cb8e6",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:04.043931Z",
          "iopub.execute_input": "2023-04-16T18:19:04.044497Z",
          "iopub.status.idle": "2023-04-16T18:19:31.389302Z",
          "shell.execute_reply.started": "2023-04-16T18:19:04.044459Z",
          "shell.execute_reply": "2023-04-16T18:19:31.388099Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Global variables\n",
        "\n",
        "ROOT_PATH = \"drive/MyDrive/NLP_Project/\"\n",
        "TRAIN_PATH_JSON = ROOT_PATH+'train-v1.1.json'\n",
        "VAL_PATH_JSON = ROOT_PATH+'dev-v1.1.json'\n",
        "# TOKEN_PRETRAINED = 'bert-base-uncased'\n",
        "TOKEN_PRETRAINED = 'distilbert-base-uncased'"
      ],
      "metadata": {
        "id": "eeRsbSJXU5Um",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:31.404831Z",
          "iopub.execute_input": "2023-04-16T18:19:31.405302Z",
          "iopub.status.idle": "2023-04-16T18:19:31.418424Z",
          "shell.execute_reply.started": "2023-04-16T18:19:31.405266Z",
          "shell.execute_reply": "2023-04-16T18:19:31.417438Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import torch\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class StanfordQADataset:\n",
        "    \n",
        "    def __init__(self, file_path,data = None):\n",
        "        self.file_path = file_path\n",
        "        self.data = data\n",
        "    \n",
        "    def load_data(self):\n",
        "        import pandas as pd\n",
        "        import json\n",
        "\n",
        "        # Load the SQuAD JSON file into a dictionary\n",
        "        with open(self.file_path, 'r') as f:\n",
        "            squad_dict = json.load(f)\n",
        "\n",
        "        # Extract the necessary information from the SQuAD dictionary\n",
        "        squad_data = []\n",
        "        for article in squad_dict['data']:\n",
        "            for paragraph in article['paragraphs']:\n",
        "                for qa in paragraph['qas']:\n",
        "                    squad_data.append({\n",
        "                        'id': qa['id'],\n",
        "                        'context': paragraph['context'],\n",
        "                        'question': qa['question'],\n",
        "                        'answer_text': qa['answers'][0]['text'],\n",
        "                        'answer_start': qa['answers'][0]['answer_start']\n",
        "                    })\n",
        "\n",
        "        # Convert the SQuAD data to a pandas dataframe\n",
        "        squad_df = pd.DataFrame(squad_data)\n",
        "\n",
        "        self.data = squad_df\n",
        "    \n",
        "    def preprocess(self, text, tokenize=True, remove_stopwords=False, stemming=False, lemmatization=False):\n",
        "        if tokenize:\n",
        "            text = word_tokenize(text)\n",
        "        if remove_stopwords:\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            text = [word for word in text if not word.lower() in stop_words]\n",
        "        if stemming:\n",
        "            stemmer = PorterStemmer()\n",
        "            text = [stemmer.stem(word) for word in text]\n",
        "        if lemmatization:\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            text = [lemmatizer.lemmatize(word) for word in text]\n",
        "        return ' '.join(text)\n",
        "    \n",
        "    def find_end_index(self):\n",
        "      end_idxs = []\n",
        "      new_start_idxs = []\n",
        "      data = self.data\n",
        "      start_idxs = data['answer_start'].values.tolist()\n",
        "      answer_texts = data['answer_text'].values.tolist()\n",
        "      contexts = data['context'].values.tolist()\n",
        "      for start, answer, context in zip(start_idxs, answer_texts, contexts):\n",
        "        end = start + len(answer)\n",
        "        if context[start: end] == answer:\n",
        "          new_start_idxs.append(start)\n",
        "          end_idxs.append(end)\n",
        "        elif context[start - 1: end - 1] == answer:\n",
        "          new_start_idxs.append(start - 1)\n",
        "          end_idxs.append(end - 1)\n",
        "        elif context[start - 2: end - 2] == answer:\n",
        "          new_start_idxs.append(start - 2)\n",
        "          end_idxs.append(end - 2)\n",
        "\n",
        "      data['new_answer_start'] = new_start_idxs\n",
        "      data['answer_end'] = end_idxs\n",
        "      return data\n",
        "      \n",
        "    def preprocess_data(self, tokenize=True, remove_stopwords=False, stemming=False, lemmatization=False):\n",
        "        if self.data is None:\n",
        "            self.load_data()\n",
        "        self.data = self.find_end_index()\n",
        "        self.data['question'] = self.data['question'].apply(lambda x: self.preprocess(x, tokenize, remove_stopwords, stemming, lemmatization))\n",
        "        self.data['context'] = self.data['context'].apply(lambda x: self.preprocess(x, tokenize, remove_stopwords, stemming, lemmatization))\n",
        "    \n",
        "    def get_word_embeddings(self, embedding_type='count', vocabulary_size=None, embedding_size=None):\n",
        "        if self.data is None:\n",
        "            self.load_data()\n",
        "        if embedding_type == 'count':\n",
        "            vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
        "            X = vectorizer.fit_transform(self.data['question'] + self.data['context']).toarray()\n",
        "        elif embedding_type == 'tfidf':\n",
        "            vectorizer = TfidfVectorizer(max_features=vocabulary_size)\n",
        "            X = vectorizer.fit_transform(self.data['question'] + self.data['context']).toarray()\n",
        "        elif embedding_type == 'word2vec':\n",
        "            sentences = [word_tokenize(text) for text in self.data['question'] + self.data['context']]\n",
        "            model = Word2Vec(sentences, size=embedding_size, window=5, min_count=vocabulary_size, workers=4)\n",
        "            X = np.zeros((len(sentences), embedding_size))\n",
        "            for i, sentence in enumerate(sentences):\n",
        "                for word in sentence:\n",
        "                    if word in model.wv.vocab:\n",
        "                        X[i] += model.wv[word]\n",
        "        else:\n",
        "            raise ValueError('Invalid embedding type.')\n",
        "        return X, {'start': data['answer_start'],'end': data['answer_end']}\n"
      ],
      "metadata": {
        "id": "xeXTRc8sVsAy",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:31.421913Z",
          "iopub.execute_input": "2023-04-16T18:19:31.422212Z",
          "iopub.status.idle": "2023-04-16T18:19:31.589176Z",
          "shell.execute_reply.started": "2023-04-16T18:19:31.422186Z",
          "shell.execute_reply": "2023-04-16T18:19:31.588195Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadData():\n",
        "  def __init__(self, train_path: str, val_path: str):\n",
        "    self.train_path = train_path\n",
        "    self.val_path = val_path\n",
        "\n",
        "  def preprocess(self, data_path):\n",
        "    process_dataset = StanfordQADataset(data_path)\n",
        "    process_dataset.load_data()\n",
        "\n",
        "    process_dataset.preprocess_data(tokenize=True, remove_stopwords=True, stemming=True, lemmatization=False)\n",
        "    return process_dataset.data\n",
        "\n",
        "  def load_data(self,trainp = 0.8):\n",
        "    temp_data= self.preprocess(self.train_path)\n",
        "    val_data = self.preprocess(self.val_path)\n",
        "    # train_data = self.find_end_index(self.preprocess(self.train_path))\n",
        "    # val_data = self.find_end_index(self.preprocess(self.val_path))\n",
        "    train_data = temp_data.sample(frac=0.8, random_state=42)\n",
        "    test_data = temp_data.drop(train_data.index)\n",
        "\n",
        "    print(f'Train Data shape: {train_data.shape}')\n",
        "    print(f'Test Data shape: {test_data.shape}')\n",
        "    print(f'Validation Data shape: {val_data.shape}')\n",
        "    return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "6NyhpQxWVvkF",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:31.591570Z",
          "iopub.execute_input": "2023-04-16T18:19:31.591867Z",
          "iopub.status.idle": "2023-04-16T18:19:31.602838Z",
          "shell.execute_reply.started": "2023-04-16T18:19:31.591840Z",
          "shell.execute_reply": "2023-04-16T18:19:31.601723Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = LoadData(TRAIN_PATH_JSON, VAL_PATH_JSON)"
      ],
      "metadata": {
        "id": "wwtWCzylV358",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:31.605140Z",
          "iopub.execute_input": "2023-04-16T18:19:31.606259Z",
          "iopub.status.idle": "2023-04-16T18:19:31.612385Z",
          "shell.execute_reply.started": "2023-04-16T18:19:31.606215Z",
          "shell.execute_reply": "2023-04-16T18:19:31.611438Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = obj.load_data()"
      ],
      "metadata": {
        "id": "s7YADpwqWy3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save them in order to retrieve later\n",
        "train_data.to_csv(ROOT_PATH+'train.csv', index=False)\n",
        "test_data.to_csv(ROOT_PATH+'test.csv', index=False)\n",
        "val_data.to_csv(ROOT_PATH+'val.csv', index=False)"
      ],
      "metadata": {
        "id": "VS5pzCoiYQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "test_data  = pd.read_csv('test.csv')\n",
        "val_data = pd.read_csv('val.csv')"
      ],
      "metadata": {
        "id": "o-TSGa9QUhRS",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:31.613864Z",
          "iopub.execute_input": "2023-04-16T18:19:31.614228Z",
          "iopub.status.idle": "2023-04-16T18:19:33.099596Z",
          "shell.execute_reply.started": "2023-04-16T18:19:31.614191Z",
          "shell.execute_reply": "2023-04-16T18:19:33.098592Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LTvf-exkm0y",
        "outputId": "6ca18d9a-ee4e-4c15-b009-a9f9a64c773e",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:33.101075Z",
          "iopub.execute_input": "2023-04-16T18:19:33.101460Z",
          "iopub.status.idle": "2023-04-16T18:19:33.108751Z",
          "shell.execute_reply.started": "2023-04-16T18:19:33.101418Z",
          "shell.execute_reply": "2023-04-16T18:19:33.107708Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(70079, 7)\n(17520, 7)\n(10570, 7)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhv9tg4Xe4Jm",
        "outputId": "a8d153c7-cc3a-4d93-e678-b081d893789f",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:33.112410Z",
          "iopub.execute_input": "2023-04-16T18:19:33.113260Z",
          "iopub.status.idle": "2023-04-16T18:19:33.182864Z",
          "shell.execute_reply.started": "2023-04-16T18:19:33.113223Z",
          "shell.execute_reply": "2023-04-16T18:19:33.181781Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Device: cuda\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a3a6f594e4464db5a7d4133f5de7a70e",
            "81b2a405800f4c9b9330b0f4074017ac",
            "d26588d1af094a179c45c36c0892f295",
            "3ba27d611b0a452b8121eb144fe8d326"
          ]
        },
        "id": "j_5dtyDSaqD4",
        "outputId": "2022ea00-9411-45fa-debd-a30a78f7b5b8",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:05:05.979062Z",
          "iopub.execute_input": "2023-04-16T18:05:05.980123Z",
          "iopub.status.idle": "2023-04-16T18:05:14.701743Z",
          "shell.execute_reply.started": "2023-04-16T18:05:05.980080Z",
          "shell.execute_reply": "2023-04-16T18:05:14.700454Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3a6f594e4464db5a7d4133f5de7a70e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b2a405800f4c9b9330b0f4074017ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d26588d1af094a179c45c36c0892f295"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ba27d611b0a452b8121eb144fe8d326"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Embedder"
      ],
      "metadata": {
        "id": "i2_hi1rYD0FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from multiprocessing import Pool\n",
        "import functools\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Preprocess():\n",
        "  def __init__(self,train_data, valid_data, test_data):\n",
        "    super(Preprocess,self).__init__()\n",
        "    self.train_data = train_data\n",
        "    self.valid_data = valid_data\n",
        "    self.test_data = test_data\n",
        "\n",
        "  def convertdatatype(self, data_fl = 1):\n",
        "    if data_fl == 1:\n",
        "      data = self.train_data\n",
        "    elif data_fl == 2:\n",
        "      data = self.valid_data\n",
        "    else: \n",
        "      data = self.test_data\n",
        "\n",
        "    train_data.dropna(inplace=True)\n",
        "    data['context'] = data['context'].astype('string')\n",
        "    data['question'] = data['question'].astype('string')\n",
        "    data['answer_text'] = data['answer_text'].astype('string')\n",
        "    if data_fl == 1:\n",
        "      self.train_data = data\n",
        "    elif data_fl == 2:\n",
        "      self.valid_data = data\n",
        "    else: \n",
        "      self.test_data = data\n",
        "\n",
        "  def tokenizedata(self,tokenizer, batch_size= 1000, data_fl = 1):\n",
        "    if data_fl == 1:\n",
        "      data = self.train_data\n",
        "    elif data_fl == 2:\n",
        "      data = self.valid_data\n",
        "    else: \n",
        "      data = self.test_data \n",
        "    q = data[\"question\"].tolist()\n",
        "    c = data[\"context\"].tolist()\n",
        "    tokenized_data = []\n",
        "    size = len(q)\n",
        "    for i in range(0,size, batch_size):\n",
        "      torch.cuda.empty_cache()\n",
        "      try:\n",
        "        inputs = tokenizer(q[i:i+batch_size], c[i:i+batch_size], padding=True, truncation=True, return_tensors='pt')\n",
        "      except Exception as e:\n",
        "        inputs = torch.zeros((batch_size, 512))\n",
        "      tokenized_data.append(inputs)\n",
        "      print(f'{i} Data rows tokenized')\n",
        "    return tokenized_data\n",
        "\n",
        "  def pad_tensors(self,tensors_list):\n",
        "    keys = list(tensors_list[0].keys())\n",
        "    new_inputs = {}\n",
        "    for key in keys:\n",
        "      max_length = max([t[key].shape[1] for t in tensors_list])\n",
        "      max_length = max_length + (max_length % 2)\n",
        "      padded_tensors = []\n",
        "      print(f'MAX length of {key} key is: {max_length}')\n",
        "      for tensor in tensors_list:\n",
        "          tensors_tuple = F.pad(tensor[key], (0, max_length - tensor[key].shape[1]), value=0)\n",
        "          padded_tensors.append(tensors_tuple)\n",
        "\n",
        "      padded_tensors = torch.cat(padded_tensors, dim=0)\n",
        "      new_inputs[key] = padded_tensors\n",
        "    return new_inputs\n",
        "\n",
        "  def train_embed_model(self,padded_data,model, batch_size = 100):\n",
        "    torch.cuda.empty_cache()\n",
        "    embeddings = []\n",
        "    tokens_tensor = padded_data['input_ids'].clone().detach().to(device)\n",
        "    attention_tensors = padded_data['attention_mask'].clone().detach().to(device)\n",
        "    size = attention_tensors.shape[0]\n",
        "    for i in range(0,size, batch_size):\n",
        "      torch.cuda.empty_cache()\n",
        "      if i%1000 == 0:\n",
        "        print(f'{i} Iterations completed')\n",
        "      input_ids = tokens_tensor[i:i+batch_size]\n",
        "      attention = attention_tensors[i:i+batch_size]\n",
        "      with torch.no_grad():\n",
        "          output = model(input_ids, attention_mask=attention)\n",
        "      batch_embeddings = output.last_hidden_state.mean(dim=1).squeeze()\n",
        "      embeddings.append(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "  def pad_indices(self, embed_data_size, data_fl = 1):\n",
        "    if data_fl == 1:\n",
        "      data = self.train_data\n",
        "    elif data_fl == 2:\n",
        "      data = self.valid_data\n",
        "    else: \n",
        "      data = self.test_data \n",
        "    start_idx = data['answer_start'].tolist()\n",
        "    end_idx = data['answer_end'].tolist()\n",
        "    pad_zero_len = embed_data_size-len(start_idx)\n",
        "    new_start_idx = start_idx + [0]*pad_zero_len\n",
        "    new_end_idx = end_idx + [0]*pad_zero_len\n",
        "\n",
        "    return torch.tensor(new_start_idx), torch.tensor(new_end_idx)\n",
        "  \n",
        "  def return_padded_embed(self, embed_data):\n",
        "    '''\n",
        "    This function padds the embedded matrix to have the same second dimenion\n",
        "    '''\n",
        "    max_length = embed_data[0].shape[0]\n",
        "    padded_tensors = []\n",
        "    for tensor in embed_data:\n",
        "        padded_tensor = F.pad(tensor, (0, 0, 0, max_length - tensor.shape[0]), value=0)\n",
        "        padded_tensors.append(padded_tensor)\n",
        "    padded_tensors = torch.stack(padded_tensors, dim=0)\n",
        "    print(f'Shape of the padded tensor is: {padded_tensors.shape}')\n",
        "    return padded_tensors\n"
      ],
      "metadata": {
        "id": "RDmwKhHZUzRG",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:33.184496Z",
          "iopub.execute_input": "2023-04-16T18:19:33.184901Z",
          "iopub.status.idle": "2023-04-16T18:19:38.668091Z",
          "shell.execute_reply.started": "2023-04-16T18:19:33.184863Z",
          "shell.execute_reply": "2023-04-16T18:19:38.666974Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_process_obj = Preprocess(train_data, val_data, test_data)"
      ],
      "metadata": {
        "id": "aKjXiVQSyFw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Data\n",
        "train_data = pre_process_obj.convertdatatype(data_fl=1)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  train_inputs = pre_process_obj.tokenizedata(tokenizer, batch_size = 1000, data_fl = 1)\n",
        "  train_padded = pre_process_obj.pad_tensors(train_inputs)\n",
        "  embed_train = pre_process_obj.train_embed_model(train_padded, model, batch_size = 300)"
      ],
      "metadata": {
        "id": "Zdi_IwWUt2eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Data\n",
        "val_data = pre_process_obj.convertdatatype(data_fl=2)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  val_inputs = pre_process_obj.tokenizedata(tokenizer,batch_size = 1000, data_fl = 2)\n",
        "  val_padded = pre_process_obj.pad_tensors(val_inputs)\n",
        "  embed_val = pre_process_obj.train_embed_model(val_padded, model, batch_size = 300)"
      ],
      "metadata": {
        "id": "wF27jfL6ERLb",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Data\n",
        "test_data = pre_process_obj.convertdatatype(data_fl=3)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  test_inputs = pre_process_obj.tokenizedata(tokenizer,batch_size = 1000, data_fl = 3)\n",
        "  test_padded = pre_process_obj.pad_tensors(test_inputs)\n",
        "  embed_test = pre_process_obj.train_model(test_padded, model, batch_size = 300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8imGKnaoWOf1",
        "outputId": "2baf9be7-cab2-4d48-c7e2-a9c61dd5c3e3",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0 Data rows tokenized\n\n1000 Data rows tokenized\n\n2000 Data rows tokenized\n\n3000 Data rows tokenized\n\n4000 Data rows tokenized\n\n5000 Data rows tokenized\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "6000 Data rows tokenized\n\n7000 Data rows tokenized\n\n8000 Data rows tokenized\n\n9000 Data rows tokenized\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "10000 Data rows tokenized\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "11000 Data rows tokenized\n\n12000 Data rows tokenized\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "13000 Data rows tokenized\n\n14000 Data rows tokenized\n\n15000 Data rows tokenized\n\n16000 Data rows tokenized\n\n17000 Data rows tokenized\n\nMAX length of input_ids key is: 512\n\nMAX length of token_type_ids key is: 512\n\nMAX length of attention_mask key is: 512\n\n0 Iterations completed\n\n3000 Iterations completed\n\n6000 Iterations completed\n\n9000 Iterations completed\n\n12000 Iterations completed\n\n15000 Iterations completed\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_train = pre_process_obj.return_padded_embed(embed_train)\n",
        "embed_test = pre_process_obj.return_padded_embed(embed_test)\n",
        "embed_val = pre_process_obj.return_padded_embed(embed_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EziB8LZMoTMX",
        "outputId": "e92e9115-aa53-4ffc-f935-5f9598a25f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shape of the padded tensor is: torch.Size([234, 300, 768])\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Saving the embedded tensors in case something fails. Hence we can retrieve them later\n",
        "torch.save(embed_train.detach().cpu(), ROOT_PATH +'train_embed_new.pt')\n",
        "torch.save(embed_test.detach().cpu(), ROOT_PATH +'test_embed_new.pt')\n",
        "torch.save(embed_val.detach().cpu(), ROOT_PATH +'val_embed_new.pt')\n"
      ],
      "metadata": {
        "id": "kQimYt5NqAZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_train = torch.load('train_embed_new.pt')\n",
        "embed_val = torch.load('val_embed_new.pt')\n",
        "embed_test = torch.load('test_embed_new.pt')"
      ],
      "metadata": {
        "id": "rgt1WQFz5XmY",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:19:55.018129Z",
          "iopub.execute_input": "2023-04-16T18:19:55.018505Z",
          "iopub.status.idle": "2023-04-16T18:19:57.439844Z",
          "shell.execute_reply.started": "2023-04-16T18:19:55.018472Z",
          "shell.execute_reply": "2023-04-16T18:19:57.438732Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_start_idx, train_end_idx = pre_process_obj.pad_indices(data_fl = 1, embed_data_size = embed_train.shape[0]*embed_train.shape[1])"
      ],
      "metadata": {
        "id": "vv8tB6kFT-Os",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:20:03.192377Z",
          "iopub.execute_input": "2023-04-16T18:20:03.193729Z",
          "iopub.status.idle": "2023-04-16T18:20:03.213703Z",
          "shell.execute_reply.started": "2023-04-16T18:20:03.193673Z",
          "shell.execute_reply": "2023-04-16T18:20:03.212611Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_start_idx, val_end_idx = pre_process_obj.pad_indices(data_fl = 2, embed_val.shape[0]*embed_val.shape[1])"
      ],
      "metadata": {
        "id": "Xzg5BprxUQeD",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:20:10.076477Z",
          "iopub.execute_input": "2023-04-16T18:20:10.077195Z",
          "iopub.status.idle": "2023-04-16T18:20:10.086993Z",
          "shell.execute_reply.started": "2023-04-16T18:20:10.077150Z",
          "shell.execute_reply": "2023-04-16T18:20:10.085799Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        h0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.bilstm(input, (h0, c0))\n",
        "        out = self.fc1(out)\n",
        "        out = nn.functional.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        start_logits, end_logits = out.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits\n"
      ],
      "metadata": {
        "id": "dlOiwfsmlMp8",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:30:53.734977Z",
          "iopub.execute_input": "2023-04-16T18:30:53.735527Z",
          "iopub.status.idle": "2023-04-16T18:30:53.761290Z",
          "shell.execute_reply.started": "2023-04-16T18:30:53.735473Z",
          "shell.execute_reply": "2023-04-16T18:30:53.759743Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters\n",
        "torch.cuda.empty_cache()\n",
        "input_size = 768\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "lr = 1e-3\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "torch.backends.cudnn.enabled = False\n",
        "\n",
        "def train_BiLSTM_Bert(embed_train, train_start_idx, train_end_idx, embed_val, val_start_idx, val_end_idx, \n",
        "                      input_size=768, hidden_size=128, num_layers=2, lr=1e-3, num_epochs=10, batch_size=5):\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    model = BiLSTM(input_size, hidden_size, num_layers)\n",
        "    model.to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        val_loss = 0.0\n",
        "        model.train()\n",
        "        for i in range(0, len(embed_train), batch_size):\n",
        "            inputs = embed_train[i:i+batch_size].to(device)\n",
        "            start_targets = train_start_idx[i:i+batch_size].float().to(device)\n",
        "            end_targets = train_end_idx[i:i+batch_size].float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            start_logits, end_logits = model(inputs)\n",
        "            start_pred = torch.argmax(start_logits, dim=1).float()\n",
        "            end_pred = torch.argmax(end_logits, dim=1).float()\n",
        "            start_loss = criterion(start_logits, start_targets)\n",
        "            end_loss = criterion(end_logits, end_targets)\n",
        "\n",
        "            loss = start_loss + end_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        train_loss /= len(embed_train)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(embed_val), batch_size):\n",
        "                inputs = embed_val[i:i+batch_size].to(device)\n",
        "                start_targets = val_start_idx[i:i+batch_size].float().to(device)\n",
        "                end_targets = val_end_idx[i:i+batch_size].float().to(device)\n",
        "\n",
        "                start_logits, end_logits = model(inputs)\n",
        "                start_pred = torch.argmax(start_logits, dim=1).float()\n",
        "                end_pred = torch.argmax(end_logits, dim=1).float()\n",
        "                start_loss = criterion(start_logits, start_targets)\n",
        "                end_loss = criterion(end_logits, end_targets)\n",
        "                loss = start_loss + end_loss\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            val_loss /= len(embed_val)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}');"
      ],
      "metadata": {
        "id": "lQ1--NoS8zQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_BiLSTM_Bert(embed_train, train_start_idx, train_end_idx, embed_val, val_start_idx, val_end_idx, \n",
        "                      input_size=768, hidden_size=128, num_layers=2, lr=1e-3, num_epochs=10, batch_size=5)"
      ],
      "metadata": {
        "id": "NCiWBb8YlchU",
        "execution": {
          "iopub.status.busy": "2023-04-16T18:44:12.878972Z",
          "iopub.execute_input": "2023-04-16T18:44:12.879412Z",
          "iopub.status.idle": "2023-04-16T18:44:16.170790Z",
          "shell.execute_reply.started": "2023-04-16T18:44:12.879371Z",
          "shell.execute_reply": "2023-04-16T18:44:16.169188Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GloVe Embedding:"
      ],
      "metadata": {
        "id": "Fwnb2Zfo58Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "def embed_text(text):\n",
        "  tokens = text.split()\n",
        "  embedding = np.zeros(100)\n",
        "  for token in tokens:\n",
        "      if token in glove.stoi:\n",
        "          embedding += glove.vectors[glove.stoi[token]].numpy()\n",
        "  if len(tokens) > 0:\n",
        "      embedding /= len(tokens)\n",
        "  return torch.tensor(embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skIvDXbs57va",
        "outputId": "8a7255f3-7344-447e-d8f3-d0f0b47a585c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:16<00:00, 24366.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train data\n",
        "train_q_glove = train_data['question'].apply(embed_text)\n",
        "train_c_glove = train_data['context'].apply(embed_text)\n",
        "train_glove = torch.cat((train_q_glove, train_c_glove), dim=1)"
      ],
      "metadata": {
        "id": "8ighrvc86RXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Val datav\n",
        "val_q_glove = val_data['question'].apply(embed_text)\n",
        "val_c_glove = val_data['context'].apply(embed_text)\n",
        "val_glove = torch.cat((val_q_glove, val_c_glove), dim=1)"
      ],
      "metadata": {
        "id": "_9JWueS-7vJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_BiLSTM_Bert(train_glove, train_start_idx, train_end_idx, val_glove, val_start_idx, val_end_idx, \n",
        "                      input_size=768, hidden_size=128, num_layers=2, lr=1e-3, num_epochs=10, batch_size=5)"
      ],
      "metadata": {
        "id": "kVDKOltT8vys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-Idf Vectorizer"
      ],
      "metadata": {
        "id": "hJNDLMLoD4ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "tfidf = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "TdJlEhPOD9qE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_train_tfidf = tfidf.fit_transform(train_data['question'].tolist())\n",
        "c_train_tfidf = tfidf.fit_transform(train_data['context'].tolist())\n",
        "\n",
        "q_train_tfidf = torch.from_numpy(q_train_tfidf.toarray()).float()\n",
        "c_train_tfidf = torch.from_numpy(c_train_tfidf.toarray()).float()\n",
        "\n",
        "train_tfidf = torch.cat((q_train_tfidf, c_train_tfidf), dim=1)"
      ],
      "metadata": {
        "id": "V7JF3rj2EPVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_val_tfidf = tfidf.fit_transform(val_data['question'].tolist())\n",
        "c_val_tfidf = tfidf.fit_transform(val_data['context'].tolist())\n",
        "\n",
        "q_val_tfidf = torch.from_numpy(q_val_tfidf.toarray()).float()\n",
        "c_val_tfidf = torch.from_numpy(c_val_tfidf.toarray()).float()\n",
        "\n",
        "val_tfidf = torch.cat((q_val_tfidf, c_val_tfidf), dim=1)"
      ],
      "metadata": {
        "id": "H3SAu42gE9IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_BiLSTM_Bert(train_tfidf, train_start_idx, train_end_idx, val_tfidf, val_start_idx, val_end_idx, \n",
        "                      input_size=768, hidden_size=128, num_layers=2, lr=1e-3, num_epochs=10, batch_size=5)"
      ],
      "metadata": {
        "id": "lCHTupfSFG42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}